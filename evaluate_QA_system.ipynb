{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Russian Jeopardy! QA System Evaluation\n",
        "\n",
        "To evaluate the answer of your Russian Jeopardy! QA system, we suggest the following steps:\n",
        "1. Use the extended dataset, that contains QuestionNumber, i.e. position of the question to which the answer is given in the Topic. That will be the rank.\n",
        "2. Normalize the predicted and ground-truth answers, for example with SpaCy.\n",
        "3. Calculate one of the metrics: cosine similarity, Damerau-Levenstein edit distance, Jaccard distance, METEOR.\n",
        "4.  Measure if the coefficient calcualted with the metric is above the minimum for cosine similarity and METEOR or below it for Damerau-Levenstein edit distance, and evaluate the answer as correct or incorrect. We suggest that for METEOR the minimum is 0.227743.\n",
        "5. For any answer given by your system, add the rank of the question to the score in case of a correct answer, and subtract the rank, otherwise. If the system has the option of abstaining from the answer, the score remains the same."
      ],
      "metadata": {
        "id": "T0qVjK2yxFZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==3.2"
      ],
      "metadata": {
        "id": "eTJwjFE_0HnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python -m spacy download ru_core_news_lg\n",
        "nlp = spacy.load(\"ru_core_news_lg\")"
      ],
      "metadata": {
        "id": "CsG92Rgq0u63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -U nltk"
      ],
      "metadata": {
        "id": "FbFgF9jRSu0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.metrics import *\n",
        "from nltk.translate import meteor_score\n",
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "tQ4MjmIY5BDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x1Y7nQ_Jrt71"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tTN_vW4qfVH",
        "outputId": "eb7bfdf7-4bc5-4a09-ba89-93695d56e498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QA_file_path = 'YOUR PATH TO FILE Russian_QA_Jeopardy_dataset_extended.csv'\n",
        "QA_dataframe = pd.read_csv(QA_file_path, sep='\\t')\n",
        "deeppavlov_100_answers_file = 'YOUR PATH TO FILE deeppavlov_0_99_answers.csv'\n",
        "deeppavlov_dataframe = pd.read_csv(deeppavlov_100_answers_file, header=None)"
      ],
      "metadata": {
        "id": "jjDSDDLvdYLA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCORE=0"
      ],
      "metadata": {
        "id": "trj7Z_QdKq4m"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  predicted_answer=deeppavlov_dataframe.loc[i, 1]\n",
        "  ground_truth=QA_dataframe.loc[i, 'Answer']\n",
        "  rank=int(QA_dataframe.loc[i, 'QuestionNumber'])\n",
        "  predicted_nlp,ground_nlp=nlp(predicted_answer), nlp(ground_truth)\n",
        "  clean_predicted, clean_ground_truth=[n.lemma_ for n in predicted_nlp if not n.is_punct], [n.lemma_ for n in ground_nlp if not n.is_punct]\n",
        "  meteor_coef=meteor_score.meteor_score([clean_predicted], clean_ground_truth)\n",
        "  if meteor_coef>0.227743:\n",
        "    SCORE+=rank\n",
        "  else:\n",
        "    SCORE-=rank"
      ],
      "metadata": {
        "id": "mr565f6DKt8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCORE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZMXR6_wMqvU",
        "outputId": "08b1bc83-754a-43cc-cea9-eac5f01cf63a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-290"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "evaluate_QA_system.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
